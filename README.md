# VAC Memory System â€” Practical SOTA LoCoMo 2025 ğŸš€  
**The worldâ€™s most accurate open-source long-term memory system for LLM agents**  
**80.1 %** accuracy on the hardest long-term memory benchmark (LoCoMo 2025, generous judge) â€” using only **gpt-4o-mini**  
**2.5 seconds** per question Â· **<$0.10** per million tokens processed Â· **100 %** strict conversation isolation  

[![Reproducible](https://img.shields.io/badge/reproducible-100%25-brightgreen)](https://github.com/vac-architector/VAC-Memory-System/actions)  
[![SOTA LoCoMo 2025](https://img.shields.io/badge/SOTA-LoCoMo_%28Generous%29-80.1%25-blueviolet)](#results)  
[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)](#quickstart)  
[![Docker Ready](https://img.shields.io/badge/Docker-Ready-2496ED)](#docker)  
[![License](https://img.shields.io/badge/License-Apache_2.0-brightgreen)](LICENSE)  

From a cell-tower climber with zero programming background to beating every published system on LoCoMo â€” in just **4.5 months**, using only Claude in the terminal and sheer determination.

This is production-ready memory you can run today, fully reproducible, with pre-built databases and indexes included.

## ğŸ† Benchmark Results (10 conversations Ã— 10 seeds = 100 runs, official generous judge)

| Conversation | Questions | Mean Accuracy | Peak Accuracy |
|--------------|---------|---------------|---------------|
| 0            | 152     | **87.5 %**    | 87.5 %        |
| 7            | 191     | **86.4 %**    | 87.2 %        |
| 2            | 152     | **85.5 %**    | 86.2 %        |
| 1            | 81      | 80.2 %        | 81.5 %        |
| 9            | 158     | 77.8 %        | 79.1 %        |
| 5            | 123     | 77.2 %        | 78.9 %        |
| 3            | 199     | 76.9 %        | 78.4 %        |
| 8            | 156     | 76.9 %        | 78.2 %        |
| 6            | 150     | 76.7 %        | 77.9 %        |
| 4            | 178     | 75.8 %        | 77.5 %        |
| **Total / Avg** | **1,540** | **80.1 %** | â€” |

Full judged logs, MD5 hashes, and reproducibility scripts in `/results/`.

**94â€“100 % ground-truth recall** Â· Only **1.7 %** pure LLM mistakes Â· Zero cross-conversation leakage

## ğŸ§  Why VAC Memory Wins

- **MCA-First Gate** â€“ custom entity/date protection that stops semantic drift before it starts  
- Hybrid retrieval (FAISS BGE-Large + BM25) with smart union  
- Final precision via BAAI/bge-reranker-v2-m3  
- Deterministic, temperature-0 answers with gpt-4o-mini  
- Per-conversation vector DBs & FAISS indexes shipped in the repo â€” truly reproducible out-of-the-box

## Contents
- `Core/` â€” protected pipeline modules (.so) + judge helper + sanitize script.
- `data/` â€” per-conv SQLite DBs, FAISS, IDMAP, `locomo10.json`, `memory.db`.
- `models/` â€” `bge-large-en-v1.5` (embeddings).
- `baseline_100 result LoCoMo/` â€” 100 judged runs (Cat1â€“4) + metrics (`metrics_full_100.csv`).
- `results/` â€” empty; new runs are written here.
- `run_test.sh`, `run_test.bat` â€” run all 10 conversations once.

## Requirements
- GPU with CUDA.
- Python 3.10+.
- OpenAI API key in `OPENAI_API_KEY` (answers + generous judge).
- Ollama running locally with `qwen2.5:14b`:
  - Install Ollama, then `ollama pull qwen2.5:14b`.
  - Ensure `OLLAMA_BASE_URL=http://localhost:11434` (default).
```

## ğŸ›  Quickstart (30 seconds)

### Linux
```bash
cd /path/to/Github
export OPENAI_API_KEY=sk-...
export OLLAMA_BASE_URL=http://localhost:11434
./run_test.sh
```

### Windows
```
cd D:\path\to\Github
set OPENAI_API_KEY=sk-...
set OLLAMA_BASE_URL=http://localhost:11434
run_test.bat
```

Outputs: `results/vac_v1_conv*.json` (+ judged if you run the judge manually).

## Optional: Judge a result
```bash
cd /path/to/Github
export OPENAI_API_KEY=sk-...
PYTHONPATH=Core \
python3 Core/gpt_official_generous_judge_from_mem0.py results/vac_v1_conv0_seed2001_*.json
python3 Core/sanitize_summary.py results/vac_v1_conv0_seed2001_*_generous_judged.json
```
(You can also copy `Core/eval_v4.26_official_generous_judge.py` into `code/` if you prefer.)

## Pipeline (high level)
1. Query Classification  
2. LLM Synonym Expansion (qwen2.5:14b)  
3. [TARGET] MCA-first filter  
4. [TARGET] FAISS top-k (semantic, BGE-large 1024D)  
5. [TARGET] BM25 top-k (sparse)  
6. Union(MCA, FAISS, BM25) -> ~112 docs  
7. [FIRE] Cross-Encoder DIRECT reranking (ALL ~112 docs -> top-15)  
8. gpt-4o-mini answer (T=0.0, max_tokens=150)

## Notes
- Directory with spaces: `baseline_100 result LoCoMo/` (quote it if scripting).
- Results dir is empty by default; create/keep `results/` for new runs. Baseline 100 runs stay in their folder.

## ğŸ“– The Story Behind It

Ten months ago I was hanging off cell towers in Chicago, then moved to Columbus with two friends to start a closet-installation business. Money disappeared fast. I was doing handyman gigs on TaskRabbit while dreaming of the RTX 4090 PC I couldnâ€™t afford. Got it on installments â€” and felt empty the moment I launched the first game.

One night I asked an AI: â€œHow hard is it to build a bot that answers my customers?â€  
It said â€œhard, but possibleâ€ and gave me the first lines of code.

That spark turned into obsession. Days and nights blurred. My girlfriend was angry â€” no money coming in, just me glued to the screen. Friends thought I was crazy: zero IT background, no degree, competing with Silicon Valley labs.

I fought hallucinations with thousands of tests. I learned to distrust even the strongest models and built my own verification loops. Four and a half months later â€” using nothing but Claude in the terminal and pure grit â€” the system youâ€™re looking at now crushed every published score on LoCoMo.

This repository is proof that impossible is just a starting point.

## ğŸš€ Ready for Production?

- Plug-and-play into any LangGraph / CrewAI / LlamaIndex agent  
- Strict conversation isolation (perfect for multi-user SaaS)  
- Extremely low cost and latency  
- Fully open weights, indexes, and logic (only tiny proprietary MCA thresholds kept secret for now)

Looking for partnerships, integrations, or investment to take this to millions of agents.

## ğŸ“¬ Contact

**Viktor** â€“ the guy who went from 150 ft cell towers to SOTA memory in 4.5 months  
Email: Vkuz02452@gmail.com Â· ViktorAdamCore@pm.me  
DMs open on X / LinkedIn / here on GitHub

Star â˜… this repo if you believe one determined human + AI can still change the game.  
Letâ€™s build the future of agent memory â€” together.
